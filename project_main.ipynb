{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpD9JNtisP0TsT/Sve7SA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinsabu/Ashwin-Sabu/blob/master/project_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('job_descriptions.csv')\n",
        "\n",
        "# Step 1: Remove duplicate and null rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Step 2: Truncate to the top 10,000 rows (if larger)\n",
        "if df.shape[0] > 10000:\n",
        "    df = df.head(10000)\n",
        "\n",
        "# Step 3: Remove specified columns\n",
        "columns_to_remove = ['Job Id', 'location', 'Country', 'latitude', 'longitude',\n",
        "                     'Work Type', 'Company Size', 'Job Posting Date', 'Preference',\n",
        "                     'Contact Person', 'Contact', 'Role', 'Job Portal',\n",
        "                     'Benefits', 'Responsibilities', 'Company']\n",
        "\n",
        "df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Optionally, save the cleaned DataFrame back to a CSV file\n",
        "df.to_csv('job_descriptions_cleaned.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkIJi88uwLAo",
        "outputId": "5c953a96-d15f-4bdb-c72e-dd7247a5c8b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Experience Qualifications Salary Range                     Job Title  \\\n",
            "0  5 to 15 Years         M.Tech    $59K-$99K  Digital Marketing Specialist   \n",
            "1  2 to 12 Years            BCA   $56K-$116K                 Web Developer   \n",
            "2  0 to 12 Years            PhD   $61K-$104K            Operations Manager   \n",
            "3  4 to 11 Years            PhD    $65K-$91K              Network Engineer   \n",
            "4  1 to 12 Years            MBA    $64K-$87K                 Event Manager   \n",
            "\n",
            "                                     Job Description  \\\n",
            "0  Social Media Managers oversee an organizations...   \n",
            "1  Frontend Web Developers design and implement u...   \n",
            "2  Quality Control Managers establish and enforce...   \n",
            "3  Wireless Network Engineers design, implement, ...   \n",
            "4  A Conference Manager coordinates and manages c...   \n",
            "\n",
            "                                              skills  \\\n",
            "0  Social media platforms (e.g., Facebook, Twitte...   \n",
            "1  HTML, CSS, JavaScript Frontend frameworks (e.g...   \n",
            "2  Quality control processes and methodologies St...   \n",
            "3  Wireless network design and architecture Wi-Fi...   \n",
            "4  Event planning Conference logistics Budget man...   \n",
            "\n",
            "                                     Company Profile  \n",
            "0  {\"Sector\":\"Diversified\",\"Industry\":\"Diversifie...  \n",
            "1  {\"Sector\":\"Financial Services\",\"Industry\":\"Com...  \n",
            "2  {\"Sector\":\"Insurance\",\"Industry\":\"Insurance: P...  \n",
            "3  {\"Sector\":\"Energy\",\"Industry\":\"Mining, Crude-O...  \n",
            "4  {\"Sector\":\"Energy\",\"Industry\":\"Energy - Oil & ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned CSV file into a DataFrame\n",
        "df_cleaned = pd.read_csv('job_descriptions_cleaned.csv')\n",
        "\n",
        "# Count unique job titles\n",
        "title_counts = df_cleaned['Job Title'].value_counts()\n",
        "\n",
        "# Convert to DataFrame and reset index\n",
        "\n",
        "df_title_counts = title_counts.reset_index()\n",
        "df_title_counts.columns = ['Job Title', 'Count']\n",
        "\n",
        "# Save to another CSV file\n",
        "df_title_counts.to_csv('job_title_counts.csv', index=False)\n"
      ],
      "metadata": {
        "id": "I4LK-suMwVXw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet') # Download wordnet for lemmatization\n",
        "\n",
        "# Load the cleaned CSV file into a DataFrame\n",
        "df = pd.read_csv('job_descriptions_cleaned.csv')\n",
        "\n",
        "# Function to clean and process text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Clean the skills column\n",
        "df['cleaned_skills'] = df['skills'].apply(clean_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['cleaned_skills'])\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Extract keywords for each job description\n",
        "def extract_keywords(text, vectorizer):\n",
        "    tfidf_matrix = vectorizer.transform([text])\n",
        "    indices = tfidf_matrix.nonzero()[1]\n",
        "    keywords = [vectorizer.get_feature_names_out()[index] for index in indices]\n",
        "    return ' '.join(keywords)\n",
        "\n",
        "# Add extracted keywords to the DataFrame\n",
        "df['skills'] = df['cleaned_skills'].apply(lambda x: extract_keywords(x, vectorizer))\n",
        "\n",
        "# Drop the temporary cleaned_skills column\n",
        "df.drop(columns=['cleaned_skills'], inplace=True)\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "df.to_csv('job_descriptions_with_keywords.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv8j58n3whkg",
        "outputId": "5ef2cecd-1284-4f59-ec8d-9cccb6f94fbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('job_descriptions_with_keywords.csv')\n",
        "\n",
        "# Function to convert experience range to numeric average\n",
        "def convert_experience_to_numeric(experience):\n",
        "    if isinstance(experience, str):\n",
        "        experience_range = experience.replace('Years', '').strip().split(' to ')\n",
        "        return (int(experience_range[0]) + int(experience_range[1])) / 2\n",
        "    return np.nan\n",
        "\n",
        "# Apply the conversion to the Experience column\n",
        "df['Experience'] = df['Experience'].apply(convert_experience_to_numeric)\n",
        "\n",
        "# Check for NaN values and remove rows with NaN values in 'Experience'\n",
        "df.dropna(subset=['Experience'], inplace=True)\n",
        "\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Truncate to the top 10,000 rows\n",
        "df = df.head(10000)\n",
        "\n",
        "# Remove specified columns\n",
        "columns_to_remove = ['Job Id', 'location', 'Country', 'latitude', 'longitude',\n",
        "                     'Work Type', 'Company Size', 'Job Posting Date', 'Preference',\n",
        "                     'Contact Person', 'Contact', 'Role', 'Job Portal',\n",
        "                     'Benefits', 'Responsibilities', 'Company']\n",
        "\n",
        "df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
        "\n",
        "# Define the features (X) and target (y)\n",
        "X = df[['Experience', 'Qualifications', 'Salary Range', 'skills']]\n",
        "y = df['Job Title']\n",
        "\n",
        "# Identify and remove classes with only one instance\n",
        "value_counts = y.value_counts()\n",
        "single_instance_classes = value_counts[value_counts == 1].index\n",
        "df = df[~y.isin(single_instance_classes)]  # Remove rows corresponding to single-instance classes\n",
        "\n",
        "# Update X and y after removing single-instance classes\n",
        "X = df[['Experience', 'Qualifications', 'Salary Range', 'skills']]\n",
        "y = df['Job Title']\n",
        "\n",
        "# Split data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Define preprocessing for numeric features (Experience)\n",
        "numeric_features = ['Experience']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Define preprocessing for categorical features (Qualifications and Salary Range)\n",
        "categorical_features = ['Qualifications', 'Salary Range']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Define preprocessing for text features (skills)\n",
        "text_features = 'skills'\n",
        "text_transformer = Pipeline(steps=[\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('text', text_transformer, text_features)\n",
        "    ])\n",
        "\n",
        "# Define the Random Forest Classifier model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create a pipeline with the preprocessor and the classifier\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', rf_classifier)\n",
        "])\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Implement GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model using grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Perform cross-validation to evaluate the model more robustly\n",
        "cv_scores = cross_val_score(grid_search.best_estimator_, X, y, cv=5)\n",
        "print(f'Cross-validation accuracy scores: {cv_scores}')\n",
        "print(f'Average cross-validation accuracy: {np.mean(cv_scores):.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YLplTNZxVqy",
        "outputId": "e1d92062-a2c7-4980-c5bf-60715ec63f95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy scores: [0.93229167 0.91099476 0.91099476 0.93193717 0.93193717]\n",
            "Average cross-validation accuracy: 0.92\n"
          ]
        }
      ]
    }
  ]
}